from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, sum
from .catalog import catalog
def extract_demographics(sparksession, catalog):
    return sparksession.read.parquet(catalog["clean/demographics"])
def store_chinese_demographics(frame, catalog):
    frame.write.parquet(catalog["business/chinese_demographics"])
# Improved aggregation function, grouped by country and province
def aggregate_inhabitants_by_province(frame):
    return (frame
            .groupBy("country", "province")
            .agg(sum(col("inhabitants")).alias("inhabitants"))
            )
def main():
    spark = SparkSession.builder.getOrCreate()
    frame = extract_demographics(spark, catalog)
    chinese_demographics = frame.filter(lower(col("country")) == "china")
    aggregated_demographics = aggregate_inhabitants_by_province(chinese_demographics)
    store_chinese_demographics(aggregated_demographics, catalog)
if __name__ == "__main__":
    main()
    
    
   
   from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax
aggregated = (purchased
              # Group rows by 'Country'
              .groupBy(col('Country'))
              .agg(
                # Calculate the average salary per group
                avg('Salary').alias('average_salary'),
                # Calculate the standard deviation per group and rename
                stddev_samp('Salary'),
                # Retain the highest salary per group and rename
                sfmax('Salary').alias('highest_salary')
              )
             )
aggregated.show()
    
    from pyspark.sql.functions import col
# Select the columns and rename the "absorption_rate" column
result = ratings.select([col("brand"),
                         col("model"),
                         col("absorption_rate").alias("absorbency")])
#Show only unique values
result.distinct().show()
  
